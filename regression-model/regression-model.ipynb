{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Lets create a simple linear regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "We will use a subset of the the chicago taxi dataset. This subset consists in 31,694 trips that happened in may 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "chicago_taxi_dataset = pd.read_csv(\"datasets/chicago_taxi_trips_may_2022.csv\", index_col=0)\n",
    "\n",
    "# divide features (X) from label (y)\n",
    "# we will choose only 3 variables to keep it simple, \n",
    "# two continuous and one categorical to make it intresting.\n",
    "X = chicago_taxi_dataset[['TRIP_MILES', 'TRIP_SECONDS', 'PAYMENT_TYPE']].copy() # dataframe\n",
    "y = chicago_taxi_dataset['FARE'].copy() # pandas series\n",
    "\n",
    "# feature engineer I: seconds to minutes\n",
    "# minutes has much bigger mean value than trip miles and fare,\n",
    "# so we will reduce it to minutes to keep numbers close\n",
    "# and help the model\n",
    "X['TRIP_MINUTES'] = X['TRIP_SECONDS']/60\n",
    "\n",
    "# print a resume of the dataset\n",
    "# count = num. of non-null values\n",
    "# unique = num. of unique values in categorical variables\n",
    "#top = Most frequent value of categorical variables (mode)\n",
    "#freq = Frequency of the top\n",
    "X.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "We will do: \n",
    "- train test split 80-20.\n",
    "- one hot encoding to categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits\n",
    "# NOTE: splits have to be done before encoding to avoid data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[['TRIP_MILES', 'TRIP_MINUTES', 'PAYMENT_TYPE']],\n",
    "                                                    y,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=100)\n",
    "\n",
    "# feature engineer II: one-hot encoding of Payment type\n",
    "# we group variables with less than 2000 events to also group\n",
    "# the category unknown which has 1,206 events. This categories will have the name\n",
    "# 'PAYMENT_TYPE_infrequent_sklearn'.\n",
    "# NOTE: handle_unknown helps to avoid problems if new cateogries appear in the test split\n",
    "encoder = OneHotEncoder(sparse_output=False, # so it returns a np matrix\n",
    "                        drop='first', # to avoid colineality\n",
    "                        min_frequency=2000, # groups all variables with less than n events\n",
    "                        handle_unknown='infrequent_if_exist') # avoid mistakes\n",
    "\n",
    "# to apply one hot encoding in the dataset, we will use ColumnTransformer\n",
    "transformer = ColumnTransformer(transformers = [('onehot', # transformation name\n",
    "                                                  encoder,  # transformation object\n",
    "                                                  ['PAYMENT_TYPE'])], # columns to apply transformation\n",
    "                                 remainder = 'passthrough') # do nothing with the other columns\n",
    "\n",
    "# apply preprocessor to datasets\n",
    "X_train_encoded = transformer.fit_transform(X_train) # apply fit transform in train set\n",
    "X_test_encoded = transformer.transform(X_test) # apply only transform in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Training is simple, we will evaluate model based on predictions in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# sklearn LinearRegression does not need any specification of categorical variables,\n",
    "# when they are one hot encoded.\n",
    "model = LinearRegression() # define model\n",
    "model.fit(X_train_encoded, y_train) # train model\n",
    "y_pred = model.predict(X_test_encoded) # predict with the outcome model\n",
    "\n",
    "# Results generator function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    metrics = {\n",
    "        'MSE': mean_squared_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)), # root mean squared error\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RÂ²': r2_score(y_test, y_pred)}\n",
    "    return metrics\n",
    "\n",
    "# Results\n",
    "training_results = evaluate_model(y_test, y_pred)\n",
    "training_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Our model gets a 95 R squared value, wich indicates excellent prediction power of the model.\n",
    "Now some visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization I: objective vs. real\n",
    "# this graph indicates good prediction if the dots look diagonal.\n",
    "plt.scatter(y_test, y_pred, alpha=0.5) # dots\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--') # diagonal line\n",
    "plt.xlabel('objective (true value)')\n",
    "plt.ylabel('prediction')\n",
    "plt.title('real vs. objective')\n",
    "plt.show()\n",
    "\n",
    "# visualization II: residuals\n",
    "# this graph indicates good prediction if the dots look horizontal.\n",
    "residuals = y_test - y_pred # residual calculation\n",
    "plt.scatter(y_pred, residuals, alpha=0.5) # dots\n",
    "plt.axhline(y=0, color='r', linestyle='--') # horizontal line\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('residuals')\n",
    "plt.title('residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Conclusion and Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Even if linnear regression is the simplest model, \n",
    "it is needed to know some characteristics of the model to get a good prediction. \n",
    "For example, we need a good feature engineer that adapts to the model.\n",
    "Finally, we need understanding of the results parameters to make conclutions.\n",
    "<br>\n",
    "**NOTES**: For next models, I recomend comparing with different characteristics, maybe with a gridsearch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Recommendation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The objective of a Recommendation Model or Recommendation System (RecSys) is to recommend relevant items for users.\n",
    "We will code a RecSys based on the article [Recommender Systems in Python 101](https://www.kaggle.com/code/gspmoreira/recommender-systems-in-python-101) of Gabriel Moreira. We will use the same variable names and some code lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "\n",
    "# modeling packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# natural lenguage tools\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We will use the \n",
    "[Deskdrop Dataset](https://www.kaggle.com/datasets/gspmoreira/articles-sharing-reading-from-cit-deskdrop)\n",
    "to follow the same steps as in the article. \n",
    "The Deskdrop Dataset contains a real sample of 12 months logs (March 2016 - February 2017) from CI&T's Internal Communication platform (DeskDrop). \n",
    "It consists of the shared articles dataset and the user interactions dataset.\n",
    "The shared articles dataset has information of the articles available on the website.\n",
    "The user interaction has information about user interation to different articles.\n",
    "<br>\n",
    "We will only consider available datasets, to avoid recommending unavaiable articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we read the datasets\n",
    "articles_df = pd.read_csv('datasets/shared_articles.csv')\n",
    "interactions_df = pd.read_csv('datasets/users_interactions.csv')\n",
    "\n",
    "# we make sure the analyzed articles are available for users\n",
    "articles_df = articles_df[articles_df['eventType'] == 'CONTENT SHARED']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "This left us with 3,047 available articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Data cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We will associate to each interation type a weight because \n",
    "some interactions represent higher interest from the user.\n",
    "We will use the same weights proposed in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# event weights\n",
    "event_type_strength = {\n",
    "   'VIEW': 1.0,\n",
    "   'LIKE': 2.0, \n",
    "   'BOOKMARK': 2.5, \n",
    "   'FOLLOW': 3.0,\n",
    "   'COMMENT CREATED': 4.0,  \n",
    "}\n",
    "\n",
    "# add a column of weight to interactions dataframe\n",
    "interactions_df['eventStrength'] = interactions_df['eventType'].apply(lambda x: event_type_strength[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "To avoid **cold start** (recommending to users with no previous interactions) we will filter for only users with 5+ interactions to different articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df of number of iteractions per user per content\n",
    "users_interactions_count_df = interactions_df.groupby(['personId', 'contentId']).size().groupby('personId').size()\n",
    "\n",
    "# df of users with 5+ interactions\n",
    "users_with_enough_interactions_df = users_interactions_count_df[users_interactions_count_df >= 5].reset_index()[['personId']]\n",
    "\n",
    "# we recreate the interactions df with selected users\n",
    "interactions_from_selected_users_df = interactions_df.merge(users_with_enough_interactions_df, \n",
    "                                                            how = 'right',\n",
    "                                                            on = 'personId')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "This left us with 1895 users.\n",
    "<br>\n",
    "For multiple interactions to the same article, we sum the interation weight of aech interaction and apply a log transform to avoid outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the log function\n",
    "def smooth_user_preference(x):\n",
    "    return math.log(1+x, 2)\n",
    "\n",
    "# we do the sum and apply the log function\n",
    "interactions_full_df = interactions_from_selected_users_df \\\n",
    "                       .groupby(['personId', 'contentId'])['eventStrength'].sum() \\\n",
    "                       .apply(smooth_user_preference).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "To begin with the modeling, we will do an 80-20 split. \n",
    "Some users are really active, so we stratify by user to avoid bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "interactions_train_df, interactions_test_df = train_test_split(interactions_full_df,\n",
    "                                   stratify=interactions_full_df['personId'], \n",
    "                                   test_size=0.20,\n",
    "                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "We will use the **Top-N accurracy** strategy to evaluate accurracy. \n",
    "We will use N=5 and N=10 and evaluate for each item the user has interacted with.\n",
    "To shorten computational time, we will limmit the recommendation analysis to 100 random selected not-viewed-by-the-user articles plus the objective one.\n",
    "Here we code the evaluation object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample size\n",
    "EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS = 100\n",
    "\n",
    "#Indexing by personId to speed up the searches during evaluation\n",
    "interactions_full_indexed_df = interactions_full_df.set_index('personId')\n",
    "interactions_train_indexed_df = interactions_train_df.set_index('personId')\n",
    "interactions_test_indexed_df = interactions_test_df.set_index('personId')\n",
    "\n",
    "# Get the list of items the user has intercated with\n",
    "def get_items_interacted(person_id, interactions_df):\n",
    "    interacted_items = interactions_df.loc[person_id]['contentId'] # select only user interactions\n",
    "    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items]) # transforms the df into a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use evaluator as an object\n",
    "class ModelEvaluator:\n",
    "\n",
    "    # method to select items the user hasn't interacted with\n",
    "    def get_not_interacted_items_sample(self, person_id, sample_size, seed=42):\n",
    "        interacted_items = get_items_interacted(person_id, interactions_full_indexed_df) # get interacted items\n",
    "        all_items = set(articles_df['contentId']) # get all items\n",
    "        non_interacted_items = all_items - interacted_items # get non-interacted items\n",
    "        random.seed(seed) # set random seed\n",
    "        non_interacted_items_sample = random.sample(list(non_interacted_items), sample_size) # here I added the list command\n",
    "        return set(non_interacted_items_sample) # return as set\n",
    "\n",
    "    # check if objective is in top n\n",
    "    def _verify_hit_top_n(self, item_id, recommended_items, topn):        \n",
    "            try: # try to find objective item\n",
    "                # here it applies a lazy evaluation:\n",
    "                index = next(i for i, c in enumerate(recommended_items) if c == item_id) # returns index if finds\n",
    "            except: # if it isnt finded\n",
    "                index = -1 # fix index \n",
    "            hit = int(index in range(0, topn)) # 1 if find; 0 alternatively\n",
    "            return hit, index\n",
    "\n",
    "    #evaluate model for each user\n",
    "    def evaluate_model_for_user(self, model, person_id):\n",
    "        interacted_values_testset = interactions_test_indexed_df.loc[person_id] # locate interacted articles for user in test set\n",
    "        if type(interacted_values_testset['contentId']) == pd.Series: # interacted is a pd.Series if coincidences <= 1\n",
    "            person_interacted_items_testset = set(interacted_values_testset['contentId']) #transform into a set\n",
    "        else: # interacted is a pd.DataFrame if there's more than one coincidence\n",
    "            person_interacted_items_testset = set([int(interacted_values_testset['contentId'])])  \n",
    "        interacted_items_count_testset = len(person_interacted_items_testset) \n",
    "\n",
    "        #use the model method to create an ordered list of recomended items\n",
    "        person_recs_df = model.recommend_items(person_id, \n",
    "                                               items_to_ignore=get_items_interacted(person_id, interactions_train_indexed_df), \n",
    "                                               topn=10000000000) # select a big number so it contemplates all not-ignored articles\n",
    "\n",
    "        # test with top5 and top10 benchmarks\n",
    "        hits_at_5_count = 0 # to count success rate\n",
    "        hits_at_10_count = 0 # to count success rate\n",
    "        \n",
    "        #For each item the user has interacted in test set\n",
    "        for item_id in person_interacted_items_testset:\n",
    "\n",
    "            # select items the user hasn't interacted with\n",
    "            non_interacted_items_sample = self.get_not_interacted_items_sample(person_id, \n",
    "                                                                               sample_size=EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS, \n",
    "                                                                               seed=item_id%(2**32))\n",
    "\n",
    "            #Combine the current interacted item with the 100 random items\n",
    "            items_to_filter_recs = non_interacted_items_sample.union(set([item_id]))\n",
    "\n",
    "            #filter recommendations list to not interacted items + current item only\n",
    "            valid_recs_df = person_recs_df[person_recs_df['contentId'].isin(items_to_filter_recs)]                    \n",
    "            valid_recs = valid_recs_df['contentId'].values\n",
    "            \n",
    "            #Verifying if the current interacted item is among the Top-N recommended items\n",
    "            hit_at_5, index_at_5 = self._verify_hit_top_n(item_id, valid_recs, 5)\n",
    "            hits_at_5_count += hit_at_5 # +1 if its in the top 5\n",
    "            hit_at_10, index_at_10 = self._verify_hit_top_n(item_id, valid_recs, 10)\n",
    "            hits_at_10_count += hit_at_10 # +1 if its in the top 10\n",
    "\n",
    "        # for each person we calculate recall = # of positives/# of trials\n",
    "        recall_at_5 = hits_at_5_count / float(interacted_items_count_testset)\n",
    "        recall_at_10 = hits_at_10_count / float(interacted_items_count_testset)\n",
    "\n",
    "        # generate a dict as answer\n",
    "        person_metrics = {'hits@5_count':hits_at_5_count, \n",
    "                          'hits@10_count':hits_at_10_count, \n",
    "                          'interacted_count': interacted_items_count_testset,\n",
    "                          'recall@5': recall_at_5,\n",
    "                          'recall@10': recall_at_10}\n",
    "        \n",
    "        return person_metrics\n",
    "\n",
    "    # \n",
    "    def evaluate_model(self, model):\n",
    "        people_metrics = [] # here we will save all users outcomes\n",
    "\n",
    "        # for each user in test set\n",
    "        for idx, person_id in enumerate(list(interactions_test_indexed_df.index.unique().values)):\n",
    "            person_metrics = self.evaluate_model_for_user(model, person_id)  # evaluate user\n",
    "            person_metrics['_person_id'] = person_id # add an element to response dict\n",
    "            people_metrics.append(person_metrics) # append each user results to a general list\n",
    "\n",
    "        # generate a df from people metrics\n",
    "        detailed_results_df = pd.DataFrame(people_metrics).sort_values('interacted_count', ascending=False)\n",
    "\n",
    "        # calculate global recall = sum of global positives / sum of global trials\n",
    "        global_recall_at_5 = detailed_results_df['hits@5_count'].sum() / float(detailed_results_df['interacted_count'].sum())\n",
    "        global_recall_at_10 = detailed_results_df['hits@10_count'].sum() / float(detailed_results_df['interacted_count'].sum())\n",
    "\n",
    "        # generate dict as answer\n",
    "        global_metrics = {'modelName': model.get_model_name(),\n",
    "                          'recall@5': global_recall_at_5,\n",
    "                          'recall@10': global_recall_at_10}    \n",
    "        return global_metrics, detailed_results_df\n",
    "\n",
    "# create model evaluator object\n",
    "model_evaluator = ModelEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Popularity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Popularity model consists in recommend the most popular non viewed elements in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityRecommender:\n",
    "\n",
    "    # fix object property name\n",
    "    MODEL_NAME = 'Popularity'\n",
    "\n",
    "    # init method\n",
    "    def __init__(self, popularity_df, items_df=None):\n",
    "        self.popularity_df = popularity_df\n",
    "        self.items_df = items_df\n",
    "\n",
    "    # get method\n",
    "    def get_model_name(self):\n",
    "        return self.MODEL_NAME\n",
    "\n",
    "    # recommend the more popular items user hasnt seen \n",
    "    def recommend_items(self, user_id, items_to_ignore=[], topn=10):\n",
    "        recommendations_df = (self.popularity_df[~self.popularity_df['contentId'].isin(items_to_ignore)] # not ignored items\n",
    "                               .sort_values('eventStrength', ascending = False) # ordered by event strength (redundant)\n",
    "                               .head(topn)) # return only top n elements\n",
    "        return recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Here we execute popularity model evaluation using top_n with n=5,10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the most popular articles\n",
    "item_popularity_df = (interactions_full_df.groupby('contentId')['eventStrength'].sum() # we sum weight of each interaction\n",
    "                                          .sort_values(ascending=False).reset_index()) # order so top1 = most popular\n",
    "\n",
    "# create object from most popular elements, i think articles_df is unnecesary\n",
    "popularity_model = PopularityRecommender(item_popularity_df, articles_df)\n",
    "\n",
    "# calculate results of training\n",
    "pop_global_metrics, pop_detailed_results_df = model_evaluator.evaluate_model(popularity_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "The results of the model are this\n",
    "- recall at top 5 = 24.18%\n",
    "- recall at top 10 = 37.25%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Content-Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "This model recommends items based on similarities with already liked items.\n",
    "We then need a way to measure the similarity between two items.\n",
    "In this case, items are articles, so we will measure article main subjects.\n",
    "We will use **TF-IDF** (term frequency–inverse document frequency), a technique that converts unstructured text into a vector structure, where eaach word in all articles are considered, every word is represented by a position in the vector, and the value measures how relevant a given word is in the text.\n",
    "After defining a vector of words with relevancy asociated, we can measure two articles similarity calculating the cosine between the relevancy vector, as the two vectors have the same words in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords ignores words with little to no semantic value, as 'the' or 'and'\n",
    "stopwords_list = stopwords.words('english') + stopwords.words('portuguese') # we used mixed languages as articles come in english mixed with portuguese.\n",
    "\n",
    "# create TF-IDF object\n",
    "vectorizer = TfidfVectorizer(analyzer='word', # tokens will be words\n",
    "                     ngram_range=(1, 2), # num of tokens to consider together. ex. text=\"I love NLP\", ngram_range=(1, 2) => [\"I\", \"love\", \"NLP\", \"I love\", \"love NLP\"]\n",
    "                     min_df=0.003, # minimum document frecuency. words that appear in less than 0.3% of the documents are ignored.\n",
    "                     max_df=0.5, # maximum document frecuency. same idea.\n",
    "                     max_features=500, # truncates the outcome to the main n words in all documents\n",
    "                     stop_words=stopwords_list) # avoid ignored words\n",
    "\n",
    "# list items id's\n",
    "item_ids = articles_df['contentId'].tolist()\n",
    "\n",
    "# apply TF-IDF with fit-transform combo\n",
    "# fit: each entry becomes a vector of tokens (tokenization)\n",
    "# transform: each vector of tokens becomes a vector of words with weight\n",
    "# returns a sparse matrix where rows are articles and columns are words\n",
    "tfidf_matrix = vectorizer.fit_transform(articles_df['title'] + \"\" + articles_df['text']) # consider also title\n",
    "\n",
    "# returns tokens-vector generated during fit\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return article-tokens weights from the sparse matrix\n",
    "def get_item_profile(item_id):\n",
    "    idx = item_ids.index(item_id) # return position of item id in list\n",
    "    item_profile = tfidf_matrix[idx:idx+1] # returns row (idx+1 is redundant)\n",
    "    return item_profile\n",
    "'''\n",
    "def get_item_profiles(ids):\n",
    "    item_profiles_list = [get_item_profile(x) for x in ids]\n",
    "    item_profiles = scipy.sparse.vstack(item_profiles_list)\n",
    "    return item_profiles\n",
    "\n",
    "def build_users_profile(person_id, interactions_indexed_df):\n",
    "    interactions_person_df = interactions_indexed_df.loc[person_id]\n",
    "    user_item_profiles = get_item_profiles(interactions_person_df['contentId'])\n",
    "    \n",
    "    user_item_strengths = np.array(interactions_person_df['eventStrength']).reshape(-1,1)\n",
    "    #Weighted average of item profiles by the interactions strength\n",
    "    user_item_strengths_weighted_avg = np.sum(user_item_profiles.multiply(user_item_strengths), axis=0) / np.sum(user_item_strengths)\n",
    "    user_profile_norm = sklearn.preprocessing.normalize(user_item_strengths_weighted_avg)\n",
    "    return user_profile_norm\n",
    "\n",
    "def build_users_profiles(): \n",
    "    interactions_indexed_df = interactions_train_df[interactions_train_df['contentId'] \\\n",
    "                                                   .isin(articles_df['contentId'])].set_index('personId')\n",
    "    user_profiles = {}\n",
    "    for person_id in interactions_indexed_df.index.unique():\n",
    "        user_profiles[person_id] = build_users_profile(person_id, interactions_indexed_df)\n",
    "    return user_profiles\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

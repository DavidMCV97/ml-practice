{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Recommendation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The objective of a Recommendation Model or Recommendation System (RecSys) is to recommend relevant items for users.\n",
    "We will code a RecSys based on the article [Recommender Systems in Python 101](https://www.kaggle.com/code/gspmoreira/recommender-systems-in-python-101) of Gabriel Moreira. We will use the same variable names and some code lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import pandas as pd # dataframe handling\n",
    "import math\n",
    "import random\n",
    "\n",
    "# modeling packages\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We will use the \n",
    "[Deskdrop Dataset](https://www.kaggle.com/datasets/gspmoreira/articles-sharing-reading-from-cit-deskdrop)\n",
    "to follow the same steps as in the article. \n",
    "The Deskdrop Dataset contains a real sample of 12 months logs (March 2016 - February 2017) from CI&T's Internal Communication platform (DeskDrop). \n",
    "It consists of the shared articles dataset and the user interactions dataset.\n",
    "The shared articles dataset has information of the articles available on the website.\n",
    "The user interaction has information about user interation to different articles.\n",
    "<br>\n",
    "We will only consider available datasets, to avoid recommending unavaiable articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "\n",
    "# we read the datasets\n",
    "articles_df = pd.read_csv('datasets/shared_articles.csv')\n",
    "interactions_df = pd.read_csv('datasets/users_interactions.csv')\n",
    "\n",
    "# we make sure the analyzed articles are available for users\n",
    "articles_df = articles_df[articles_df['eventType'] == 'CONTENT SHARED']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "This left us with 3,047 available articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Data cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We will associate to each interation type a weight because \n",
    "some interactions represent higher interest from the user.\n",
    "We will use the same weights proposed in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# event weights\n",
    "event_type_strength = {\n",
    "   'VIEW': 1.0,\n",
    "   'LIKE': 2.0, \n",
    "   'BOOKMARK': 2.5, \n",
    "   'FOLLOW': 3.0,\n",
    "   'COMMENT CREATED': 4.0,  \n",
    "}\n",
    "\n",
    "# add a column of weight to interactions dataframe\n",
    "interactions_df['eventStrength'] = interactions_df['eventType'].apply(lambda x: event_type_strength[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "To avoid **cold start** (recommending to users with no previous interactions) we will filter for only users with 5+ interactions to different articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df of number of iteractions per user per content\n",
    "users_interactions_count_df = interactions_df.groupby(['personId', 'contentId']).size().groupby('personId').size()\n",
    "\n",
    "# df of users with 5+ interactions\n",
    "users_with_enough_interactions_df = users_interactions_count_df[users_interactions_count_df >= 5].reset_index()[['personId']]\n",
    "\n",
    "# we recreate the interactions df with selected users\n",
    "interactions_from_selected_users_df = interactions_df.merge(users_with_enough_interactions_df, \n",
    "                                                            how = 'right',\n",
    "                                                            on = 'personId')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "This left us with 1895 users.\n",
    "<br>\n",
    "For multiple interactions to the same article, we sum the interation weight of aech interaction and apply a log transform to avoid outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the log function\n",
    "def smooth_user_preference(x):\n",
    "    return math.log(1+x, 2)\n",
    "\n",
    "# we do the sum and apply the log function\n",
    "interactions_full_df = interactions_from_selected_users_df \\\n",
    "                       .groupby(['personId', 'contentId'])['eventStrength'].sum() \\\n",
    "                       .apply(smooth_user_preference).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "To begin with the modeling, we will do an 80-20 split. \n",
    "Some users are really active, so we stratify by user to avoid bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "interactions_train_df, interactions_test_df = train_test_split(interactions_full_df,\n",
    "                                   stratify=interactions_full_df['personId'], \n",
    "                                   test_size=0.20,\n",
    "                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "We will use the **Top-N accurracy** strategy to evaluate accurracy. \n",
    "We will use N=5 and N=10 and evaluate for each item the user has interacted with.\n",
    "To shorten computational time, we will limmit the recommendation analysis to 100 random selected not-viewed-by-the-user articles plus the objective one.\n",
    "Here we code the evaluation object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample size\n",
    "EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS = 100\n",
    "\n",
    "#Indexing by personId to speed up the searches during evaluation\n",
    "interactions_full_indexed_df = interactions_full_df.set_index('personId')\n",
    "interactions_train_indexed_df = interactions_train_df.set_index('personId')\n",
    "interactions_test_indexed_df = interactions_test_df.set_index('personId')\n",
    "\n",
    "# Get the list of items the user has intercated with\n",
    "def get_items_interacted(person_id, interactions_df):\n",
    "    interacted_items = interactions_df.loc[person_id]['contentId'] # select only user interactions\n",
    "    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items]) # transforms the df into a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use evaluator as an object\n",
    "class ModelEvaluator:\n",
    "\n",
    "    # method to select items the user hasn't interacted with\n",
    "    def get_not_interacted_items_sample(self, person_id, sample_size, seed=42):\n",
    "        interacted_items = get_items_interacted(person_id, interactions_full_indexed_df) # get interacted items\n",
    "        all_items = set(articles_df['contentId']) # get all items\n",
    "        non_interacted_items = all_items - interacted_items # get non-interacted items\n",
    "        random.seed(seed) # set random seed\n",
    "        non_interacted_items_sample = random.sample(list(non_interacted_items), sample_size) # here I added the list command\n",
    "        return set(non_interacted_items_sample) # return as set\n",
    "\n",
    "    # check if objective is in top n\n",
    "    def _verify_hit_top_n(self, item_id, recommended_items, topn):        \n",
    "            try: # try to find objective item\n",
    "                # here it applies a lazy evaluation:\n",
    "                index = next(i for i, c in enumerate(recommended_items) if c == item_id) # returns index if finds\n",
    "            except: # if it isnt finded\n",
    "                index = -1 # fix index \n",
    "            hit = int(index in range(0, topn)) # 1 if find; 0 alternatively\n",
    "            return hit, index\n",
    "\n",
    "    #evaluate model for each user\n",
    "    def evaluate_model_for_user(self, model, person_id):\n",
    "        interacted_values_testset = interactions_test_indexed_df.loc[person_id] # locate interacted articles for user in test set\n",
    "        if type(interacted_values_testset['contentId']) == pd.Series: # interacted is a pd.Series if coincidences <= 1\n",
    "            person_interacted_items_testset = set(interacted_values_testset['contentId']) #transform into a set\n",
    "        else: # interacted is a pd.DataFrame if there's more than one coincidence\n",
    "            person_interacted_items_testset = set([int(interacted_values_testset['contentId'])])  \n",
    "        interacted_items_count_testset = len(person_interacted_items_testset) \n",
    "\n",
    "        #use the model method to create an ordered list of recomended items\n",
    "        person_recs_df = model.recommend_items(person_id, \n",
    "                                               items_to_ignore=get_items_interacted(person_id, interactions_train_indexed_df), \n",
    "                                               topn=10000000000) # need to check this later...\n",
    "\n",
    "        # test with top5 and top10 benchmarks\n",
    "        hits_at_5_count = 0 # to count success rate\n",
    "        hits_at_10_count = 0 # to count success rate\n",
    "        \n",
    "        #For each item the user has interacted in test set\n",
    "        for item_id in person_interacted_items_testset:\n",
    "\n",
    "            # select items the user hasn't interacted with\n",
    "            non_interacted_items_sample = self.get_not_interacted_items_sample(person_id, \n",
    "                                                                               sample_size=EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS, \n",
    "                                                                               seed=item_id%(2**32))\n",
    "\n",
    "            #Combine the current interacted item with the 100 random items\n",
    "            items_to_filter_recs = non_interacted_items_sample.union(set([item_id]))\n",
    "\n",
    "            #filter recommendations list to not interacted items + current item only\n",
    "            valid_recs_df = person_recs_df[person_recs_df['contentId'].isin(items_to_filter_recs)]                    \n",
    "            valid_recs = valid_recs_df['contentId'].values\n",
    "            \n",
    "            #Verifying if the current interacted item is among the Top-N recommended items\n",
    "            hit_at_5, index_at_5 = self._verify_hit_top_n(item_id, valid_recs, 5)\n",
    "            hits_at_5_count += hit_at_5 # +1 if its in the top 5\n",
    "            hit_at_10, index_at_10 = self._verify_hit_top_n(item_id, valid_recs, 10)\n",
    "            hits_at_10_count += hit_at_10 # +1 if its in the top 10\n",
    "\n",
    "        # for each person we calculate recall = # of positives/# of trials\n",
    "        recall_at_5 = hits_at_5_count / float(interacted_items_count_testset)\n",
    "        recall_at_10 = hits_at_10_count / float(interacted_items_count_testset)\n",
    "\n",
    "        # generate a dict as answer\n",
    "        person_metrics = {'hits@5_count':hits_at_5_count, \n",
    "                          'hits@10_count':hits_at_10_count, \n",
    "                          'interacted_count': interacted_items_count_testset,\n",
    "                          'recall@5': recall_at_5,\n",
    "                          'recall@10': recall_at_10}\n",
    "        \n",
    "        return person_metrics\n",
    "\n",
    "    # \n",
    "    def evaluate_model(self, model):\n",
    "        people_metrics = [] # here we will save all users outcomes\n",
    "\n",
    "        # for each user in test set\n",
    "        for idx, person_id in enumerate(list(interactions_test_indexed_df.index.unique().values)):\n",
    "            person_metrics = self.evaluate_model_for_user(model, person_id)  # evaluate user\n",
    "            person_metrics['_person_id'] = person_id # add an element to response dict\n",
    "            people_metrics.append(person_metrics) # append each user results to a general list\n",
    "\n",
    "        # generate a df from people metrics\n",
    "        detailed_results_df = pd.DataFrame(people_metrics).sort_values('interacted_count', ascending=False)\n",
    "\n",
    "        # calculate global recall = sum of global positives / sum of global trials\n",
    "        global_recall_at_5 = detailed_results_df['hits@5_count'].sum() / float(detailed_results_df['interacted_count'].sum())\n",
    "        global_recall_at_10 = detailed_results_df['hits@10_count'].sum() / float(detailed_results_df['interacted_count'].sum())\n",
    "\n",
    "        # generate dict as answer\n",
    "        global_metrics = {'modelName': model.get_model_name(),\n",
    "                          'recall@5': global_recall_at_5,\n",
    "                          'recall@10': global_recall_at_10}    \n",
    "        return global_metrics, detailed_results_df\n",
    "\n",
    "# create model evaluator object\n",
    "model_evaluator = ModelEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Popularity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Popularity model consists in recommend the most popular non viewed elements in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the most popular articles\n",
    "item_popularity_df = (interactions_full_df.groupby('contentId')['eventStrength'].sum() # we sum weight of each interaction\n",
    "                                          .sort_values(ascending=False).reset_index()) # order so top1 = most popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityRecommender:\n",
    "\n",
    "    # fix object property name\n",
    "    MODEL_NAME = 'Popularity'\n",
    "\n",
    "    # init method\n",
    "    def __init__(self, popularity_df, items_df=None):\n",
    "        self.popularity_df = popularity_df\n",
    "        self.items_df = items_df\n",
    "\n",
    "    # get method\n",
    "    def get_model_name(self):\n",
    "        return self.MODEL_NAME\n",
    "\n",
    "    # recommend the more popular items user hasnt seen \n",
    "    def recommend_items(self, user_id, items_to_ignore=[], topn=10):\n",
    "        recommendations_df = (self.popularity_df[~self.popularity_df['contentId'].isin(items_to_ignore)] # not ignored items\n",
    "                               .sort_values('eventStrength', ascending = False) # ordered by event strength (redundant)\n",
    "                               .head(topn)) # return only top n elements\n",
    "        return recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Here we execute popularity model evaluation using top_n with n=5,10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object from most popular elements, i think articles_df is unnecesary\n",
    "popularity_model = PopularityRecommender(item_popularity_df, articles_df)\n",
    "\n",
    "# calculate results of training\n",
    "pop_global_metrics, pop_detailed_results_df = model_evaluator.evaluate_model(popularity_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_global_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
